{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset generator\n",
    "\n",
    "- üåç **Task**: Generate realistic synthetic datasets\n",
    "- üéØ **Supported Data Types**: Tabular, Text, Time-series\n",
    "- üß† **Models**: GPT (OpenAI) , Claude (Anthropic), CodeQwen1.5-7B-Chat (via Hugging Face Inference)\n",
    "- üöÄ **Tools**: Python, pandas, numpy, Gradio UI, OpenAI / Anthropic / HuggingFace APIs\n",
    "- üì§ **Output Formats**: JSON and CSV file\n",
    "- üßë‚Äçüíª **Skill Level**: Intermediate\n",
    "- ‚öôÔ∏è **Hardware**: ‚úÖ CPU is sufficient ‚Äî no GPU required\n",
    "\n",
    "üéØ **How It Works**\n",
    "\n",
    "1Ô∏è‚É£ Define your business problem or dataset topic.  \n",
    "2Ô∏è‚É£ Choose the dataset type, output format, model, and number of samples.  \n",
    "3Ô∏è‚É£ Get a ready-to-use synthetic dataset, instantly downloadable in your preferred format!\n",
    "\n",
    "üõ†Ô∏è **Requirements**  \n",
    "- üîë OpenAI API Key (for GPT)  \n",
    "- üîë Anthropic API Key (for Claude)  \n",
    "- üîë Hugging Face Token + Endpoint (for CodeQwen1.5-7B-Chat via HF Inference)  \n",
    "  e.g. `https://w0e2ze0xyrjnhx0y.us-east-1.aws.endpoints.huggingface.cloud`\n",
    "\n",
    "‚öôÔ∏è **Customizable by user**  \n",
    "- ü§ñ Selected model: GPT / Claude / CodeQwen  \n",
    "- üìú `system_prompt`: Controls model behavior (concise, accurate, structured)  \n",
    "- üí¨ `user_prompt`: Dynamic ‚Äî includes business problem, data type, format, and path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os   \n",
    "import sys\n",
    "import subprocess\n",
    "import gradio as gr\n",
    "import openai\n",
    "import anthropic\n",
    "from dotenv import load_dotenv \n",
    "from openai import OpenAI     \n",
    "import anthropic          \n",
    "import gradio as gr  \n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå OpenAI API Key is missing!\")\n",
    "\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå Anthropic API Key is missing!\")\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "## Hugging Face Models\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå Hagging Face Token is missing!\")\n",
    "    \n",
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "CODE_QWEN_URL = \"https://w0e2ze0xyrjnhx0y.us-east-1.aws.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a helpful assistant whose main purpose is to generate datasets for business problems.\n",
    "\n",
    "Be less verbose.\n",
    "Be accurate and concise.\n",
    "\n",
    "The user will describe a business problem. Based on this, you must generate a synthetic dataset that fits the context.\n",
    "\n",
    "The dataset should be saved in a specific format such as CSV, JSON ‚Äî the desired format will be specified by the user.\n",
    "\n",
    "The dependencies for python code should include only standard python libraries such as numpy, pandas and built-in libraries. \n",
    "\n",
    "When saving a DataFrame to JSON using `to_json()`, do not use the `encoding` parameter. Instead, manually open the file with `open()` and specify the encoding. Then pass the file object to `to_json()`.\n",
    "\n",
    "Return only the Python code that generates and saves the dataset.\n",
    "After saving the file, print the code that was executed and a message confirming the dataset was generated successfully.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt(**input_data):\n",
    "    user_prompt = f\"\"\"\n",
    "        Generate a synthetic {input_data[\"dataset_type\"].lower()} dataset in {input_data[\"output_format\"].upper()} format.       \n",
    "        Business problem: {input_data[\"business_problem\"]}\n",
    "        Samples: {input_data[\"num_samples\"]}\n",
    "        \"\"\"\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call API for Closed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(user_prompt):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\",\"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def stream_claude(user_prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply\n",
    "            print(text, end=\"\", flush=True)\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call API for Open Models (Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_code_qwen(user_prompt):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\",\"content\": user_prompt},\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    client = InferenceClient(CODE_QWEN_URL, token=hf_token)\n",
    "    stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        yield result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the model and generate the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_inputs(model, **input_data):\n",
    "    # print(\"üîç input_data received:\", input_data)\n",
    "    user_prompt_str = user_prompt(**input_data)\n",
    "    \n",
    "    if model == \"GPT\":\n",
    "        result = stream_gpt(user_prompt_str)\n",
    "    elif model == \"Claude\":\n",
    "        result = stream_claude(user_prompt_str)\n",
    "    elif model == \"Code Qwen\":\n",
    "        result = stream_code_qwen(user_prompt_str)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    \n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_generate(business_problem, dataset_type, dataset_format, num_samples, model):\n",
    "    input_data = {\n",
    "        \"business_problem\": business_problem,\n",
    "        \"dataset_type\": dataset_type,\n",
    "        \"output_format\": dataset_format,\n",
    "        \"num_samples\": num_samples,\n",
    "    }\n",
    "\n",
    "    response = generate_from_inputs(model, **input_data)\n",
    "    for chunk in response:\n",
    "        yield chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract python code from the LLM output and execute it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_code(text):\n",
    "    match = re.search(r\"```python(.*?)```\", text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        code = match.group(0).strip()\n",
    "    else:\n",
    "        code = \"\"\n",
    "        print(\"No matching substring found.\")\n",
    "\n",
    "    return code.replace(\"```python\\n\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "\n",
    "def execute_code_in_virtualenv(text, python_interpreter=sys.executable):\n",
    "    if not python_interpreter:\n",
    "        raise EnvironmentError(\"Python interpreter not found in the specified virtual environment.\")\n",
    "\n",
    "    code_str = extract_code(text)\n",
    "    command = [python_interpreter, '-c', code_str]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        stdout = result.stdout\n",
    "        return stdout\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Execution error:\\n{e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_output_format(dataset_type):\n",
    "    if dataset_type in [\"Tabular\", \"Time-series\"]:\n",
    "        return gr.update(choices=[\"JSON\", \"csv\"], value=\"JSON\")\n",
    "    elif dataset_type == \"Text\":\n",
    "        return gr.update(choices=[\"JSON\"], value=\"JSON\")\n",
    "        \n",
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"## Create a dataset for a business problem\")\n",
    "    \n",
    "    with gr.Column():\n",
    "        business_problem = gr.Textbox(label=\"Business problem\", lines=2)\n",
    "        dataset_type = gr.Dropdown(\n",
    "            [\"Tabular\", \"Time-series\", \"Text\"], label=\"Dataset type\"\n",
    "        )\n",
    "        \n",
    "        output_format = gr.Dropdown( choices=[\"JSON\", \"csv\"], value=\"JSON\",label=\"Output Format\")\n",
    "        \n",
    "        num_samples = gr.Number(label=\"Number of samples (for tabular and time-series data)\", value=10, precision=0)\n",
    "        \n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
    "        \n",
    "        dataset_type.change(update_output_format,inputs=[dataset_type], outputs=[output_format])\n",
    "    \n",
    "    with gr.Row():\n",
    "        dataset_run = gr.Button(\"Create a dataset\")\n",
    "        code_run = gr.Button(\"Execute code for a dataset\")\n",
    "   \n",
    "    with gr.Row():\n",
    "        dataset_out = gr.Textbox(label=\"Generated Dataset\")\n",
    "        code_out = gr.Textbox(label=\"Executed code\")\n",
    "    \n",
    "    dataset_run.click(\n",
    "        handle_generate,\n",
    "        inputs=[business_problem, dataset_type, output_format, num_samples, model],\n",
    "        outputs=[dataset_out]\n",
    "    )\n",
    "    \n",
    "    code_run.click(\n",
    "        execute_code_in_virtualenv,\n",
    "        inputs=[dataset_out],\n",
    "        outputs=[code_out]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the Python code to generate and save the synthetic dataset as requested:\n",
      "\n",
      "```python\n",
      " as pd pandas\n",
      " as np numpy\n",
      "\n",
      " random seed for reproducibility\n",
      "seed(42)m.\n",
      "\n",
      "# Generate data\n",
      " = ['Clothes', 'Cosmetics']\n",
      "products = {\n",
      " ['T-shirt', 'Jeans', 'Dress', 'Jacket', 'Skirt'],\n",
      "etics': ['Lipstick', 'Foundation', 'Mascara', 'Eyeshadow', 'Blush']\n",
      "}\n",
      "\n",
      " []a =\n",
      " in range(10):\n",
      ".random.choice(categories)\n",
      " = np.random.choice(products[category])\n",
      " np.round(np.random.uniform(10, 100), 2)\n",
      "d([category, product, price])\n",
      "\n",
      "# Create DataFrame\n",
      "DataFrame(data, columns=['Category', 'Product', 'Price'])\n",
      "\n",
      " CSVve to\n",
      "_csv('clothes_cosmetics_dataset.csv', index=False)\n",
      "\n",
      " generated and saved as 'clothes_cosmetics_dataset.csv'\")\n",
      "(\"\\nGenerated DataFrame:\")\n",
      ")rint(df\n",
      "```\n",
      "\n",
      " code will generate a CSV file named 'clothes_cosmetics_dataset.csv' with 10 samples of clothes and cosmetic products, including their categories and prices.Here's the Python code to generate a synthetic time-series dataset for clothes and cosmetic products with prices, and save it in JSON format:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      " datetime import datetime, timedelta\n",
      "\n",
      " random seed for reproducibility\n",
      "seed(42)m.\n",
      "\n",
      "# Generate data\n",
      "_samples = 10\n",
      "_date = datetime(2023, 1, 1)\n",
      " = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
      "\n",
      "T-shirt', 'Jeans', 'Dress', 'Lipstick', 'Foundation', 'Mascara']\n",
      " = ['Clothes', 'Clothes', 'Clothes', 'Cosmetics', 'Cosmetics', 'Cosmetics']\n",
      "\n",
      "data = []\n",
      " in dates:\n",
      " product, category in zip(products, categories):\n",
      "price = np.round(np.random.uniform(10, 100), 2)\n",
      "d({     data.appen\n",
      "': date.strftime('%Y-%m-%d'),\n",
      "product': product,\n",
      ",           'category': category\n",
      "': price    'price\n",
      "        })\n",
      "\n",
      "# Create DataFrame\n",
      "(data)d.DataFrame\n",
      "\n",
      " Save to JSON\n",
      " open('clothes_cosmetics_prices.json', 'w', encoding='utf-8') as f:\n",
      "json(f, orient='records', date_format='iso', indent=2)\n",
      "\n",
      " code executed successfully.\")\n",
      "d and saved as 'clothes_cosmetics_prices.json'.\")\n",
      "```\n",
      "\n",
      " generates a synthetic time-series dataset with 10 samples for clothes and cosmetic products, including their prices, and saves it in JSON format."
     ]
    }
   ],
   "source": [
    "ui.launch(inbrowser=True, allowed_paths=[file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

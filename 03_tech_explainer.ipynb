{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e907206-4c13-4698-91c6-9ca1c32be8e7",
   "metadata": {},
   "source": [
    "# TechExplainAI\n",
    "AI-driven tool that provides concise, structured explanations for technical questions and code snippets.\n",
    "\n",
    "https://github.com/lisek75/nlp_llms_notebook/blob/main/03_tech_explainer.ipynb\n",
    "\n",
    "- 🌍 Task: AI-powered technical explanation generator\n",
    "- 🧠 Model: OpenAI's ``GPT-4o-mini``, Ollama's ``Llama 3.2``\n",
    "- 📌 Output Format: Markdown with real-time streaming\n",
    "- 🧑‍💻 Skill Level: Beginner\n",
    "- 🔄 Interaction Mode: User enters a technical question → AI generates a structured, concise explanation\n",
    "- 🎯 Purpose: Quickly explain technical concepts and Python code snippets\n",
    "- 🔧 Customization: Users can modify the models, prompts, and formatting as needed\n",
    "\n",
    "🛠️ Requirements\n",
    "- ⚙️ Hardware: ✅ CPU is sufficient — no GPU required\n",
    "- 🔑 OpenAI API Key\n",
    "- Install Ollama and pull llama3\n",
    "\n",
    "📢 Find more LLM notebooks on my GitHub repository: https://github.com/lisek75/nlp_llms_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f743c87a-ed80-43d5-84ad-c78c8bdacb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hello, I am your personal technical tutor. Enter your question:  explain llm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "🚀🤖🚀 Response from OpenAI GPT-4o-mini 🚀🤖🚀\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LLM stands for \"Large Language Model.\" It refers to a type of artificial intelligence (AI) model designed to understand and generate human-like text. \n",
       "\n",
       "### Key Features:\n",
       "1. **Scale**: Trained on vast amounts of text data (millions to billions of parameters).\n",
       "2. **Capabilities**:\n",
       "   - **Text Generation**: Produces coherent and contextually relevant sentences.\n",
       "   - **Comprehension**: Understands context, allowing for responses to prompts or questions.\n",
       "   - **Language Translation**: Translates text from one language to another.\n",
       "   - **Summarization**: Condenses longer pieces of text into shorter summaries.\n",
       "\n",
       "### Examples:\n",
       "- **OpenAI's GPT-3**: One of the most well-known LLMs, capable of various text-based tasks.\n",
       "- **BERT**: Focuses on understanding the context within sentences.\n",
       "\n",
       "### Applications:\n",
       "- Chatbots\n",
       "- Content creation\n",
       "- Code generation\n",
       "- Customer support\n",
       "\n",
       "In summary, LLMs are powerful tools in natural language processing, leveraging scale to perform various language-related tasks effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "🔥✨🔥 Response from Llama 3.2 🔥✨🔥\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**LLM (Large Language Model)**\n",
       "\n",
       "A Large Language Model is a type of artificial intelligence (AI) designed to process and understand human language.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "1. **Deep Learning**: LLMs use deep learning techniques, such as neural networks, to analyze vast amounts of text data.\n",
       "2. **Transformers**: Many LLMs employ transformer architectures, which allow them to handle sequential data like text.\n",
       "3. **Self-Supervised Learning**: LLMs are trained on large datasets without labeled examples, relying on self-supervised learning techniques.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "1. **Text Input**: The model takes in a piece of text as input.\n",
       "2. **Embedding Layer**: The input text is converted into numerical representations (embeddings) using an embedding layer.\n",
       "3. **Encoder**: The embeddings are then processed through multiple layers to extract meaningful features from the input text.\n",
       "4. **Decoder**: The extracted features are fed into the decoder, which generates a response based on the input text.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "1. **Language Translation**\n",
       "2. **Text Summarization**\n",
       "3. **Chatbots and Conversational AI**\n",
       "4. **Question Answering Systems**\n",
       "5. **Content Generation**\n",
       "\n",
       "**Example Use Case:**\n",
       "\n",
       "```python\n",
       "import torch\n",
       "\n",
       "# Define a simple LLM model\n",
       "class SimpleLLM(torch.nn.Module):\n",
       "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
       "        super(SimpleLLM, self).__init__()\n",
       "        self.embedding = torch.nn.Embedding(10000, embedding_dim)\n",
       "        self.encoder = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
       "        self.decoder = torch.nn.Linear(hidden_dim, output_dim)\n",
       "\n",
       "    def forward(self, input_text):\n",
       "        # Embed the input text\n",
       "        embeddings = self.embedding(input_text)\n",
       "        \n",
       "        # Process the embeddings through the encoder\n",
       "        h0 = torch.zeros(1, 1, hidden_dim).to(embeddings.device)\n",
       "        c0 = torch.zeros(1, 1, hidden_dim).to(embeddings.device)\n",
       "        out, _ = self.encoder(embeddings, (h0, c0))\n",
       "        \n",
       "        # Generate a response using the decoder\n",
       "        response = self.decoder(out[:, -1, :])\n",
       "        return response\n",
       "\n",
       "# Initialize the model and input text\n",
       "model = SimpleLLM(embedding_dim=128, hidden_dim=256, output_dim=512)\n",
       "input_text = \"Hello, how are you?\"\n",
       "```\n",
       "\n",
       "This example demonstrates a basic LLM model using PyTorch. Note that this is a simplified version of a real-world LLM, and actual models require more complex architectures and larger datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import ollama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, update_display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Set up OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Please set your OpenAI API key in environment variables.\")\n",
    "\n",
    "# Constants\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "# Prompt user for question (until input is provided)\n",
    "while True:\n",
    "    question = input(\"Hello, I am your personal technical tutor. Enter your question: \").strip()\n",
    "    if question:\n",
    "        break  # Proceed only if a valid question is entered\n",
    "    print(\"Question cannot be empty. Please enter a question.\")\n",
    "\n",
    "# Common user prompt\n",
    "user_prompt = f\"\"\"\n",
    "Please give a detailed explanation to the following question: {question}. \n",
    "Be less verbose.\n",
    "Provide a clear and concise explanation without unnecessary elaboration.\n",
    "\"\"\"\n",
    "\n",
    "# Common system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant that explains Python code in a clear and concise manner. Provide structured explanations and examples when necessary.\n",
    "Be less verbose.\n",
    "\"\"\"\n",
    "\n",
    "def ask_openai():\n",
    "    \"\"\"Gets response from OpenAI's GPT model with streaming.\"\"\"\n",
    "    print(\"\\n\\n\\n🚀🤖🚀 Response from OpenAI GPT-4o-mini 🚀🤖🚀\")\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response_stream = client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in response_stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "def ask_ollama():\n",
    "    \"\"\"Gets response from Ollama's Llama 3.2 model with streaming.\"\"\"\n",
    "    print(\"\\n\\n\\n🔥✨🔥 Response from Llama 3.2 🔥✨🔥\\n\")\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    full_text = \"\"\n",
    "    for chunk in response:\n",
    "        if \"message\" in chunk:\n",
    "                content = chunk[\"message\"][\"content\"] or \"\"\n",
    "                full_text += content\n",
    "                update_display(Markdown(full_text), display_id=display_handle.display_id)\n",
    "\n",
    "# Call the functions\n",
    "ask_openai()\n",
    "ask_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf06c7f-2c57-4fcb-baac-08c4cf76201e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
